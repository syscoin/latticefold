use ark_std::{
    log2,
    ops::{Mul, Sub},
    One,
};
use latticefold::{
    transcript::Transcript,
    utils::sumcheck::{
        utils::eq_eval,
        MLSumcheck, Proof, SumCheckError,
    },
};
use stark_rings::{unit_monomial, CoeffRing, OverField, PolyRing, Ring, Zq};
use stark_rings_linalg::SparseMatrix;
use std::sync::Arc;
use std::time::Instant;

use crate::{
    rgchk::{Dcom, Rg},
    streaming_sumcheck::{StreamingMleEnum, StreamingSumcheck},
    utils::{short_challenge, tensor, tensor_product},
};

#[derive(Clone, Debug)]
pub struct Cm<R: PolyRing> {
    pub rg: Rg<R>,
}

// eval over r_o of [tau (a), m_tau (b), f (c), h (u)] over 1 + n_lin
#[derive(Clone, Debug)]
pub struct InstanceEvals<R>(Vec<[R; 4]>);

impl<R> InstanceEvals<R> {
    pub(crate) fn rows(&self) -> &[[R; 4]] {
        &self.0
    }
}

#[derive(Clone, Debug)]
pub struct CmProof<R: PolyRing> {
    pub dcom: Dcom<R>,
    pub comh: Vec<Vec<R>>,
    pub sumcheck_proofs: (Proof<R>, Proof<R>),
    pub evals: (Vec<InstanceEvals<R>>, Vec<InstanceEvals<R>>),
}

#[derive(Clone, Debug)]
pub struct Com<R> {
    pub g: Vec<Vec<R>>,
    pub x: ComX<R>,
}

#[derive(Clone, Debug)]
pub struct ComX<R> {
    pub cm_g: Vec<Vec<R>>,
    pub ro: Vec<(R, R)>,
    pub vo: Vec<Vec<(R, R)>>,
}

impl<R: CoeffRing> Cm<R>
where
    R::BaseRing: Zq,
{
    pub fn prove(
        &self,
        M: &[SparseMatrix<R>],
        transcript: &mut impl Transcript<R>,
    ) -> (Com<R>, CmProof<R>) {
        let profile = std::env::var("LF_PLUS_PROFILE").ok().as_deref() == Some("1");
        let t_total = Instant::now();

        let k = self.rg.dparams.k;
        let d = R::dimension();
        let dp = R::dimension() / 2;
        let l = self.rg.dparams.l;
        let n = self.rg.instances[0].tau.len();

        if profile {
            #[cfg(feature = "parallel")]
            println!(
                "[LF+ Cm::prove] start: n={} nvars={} Mlen={} rayon_threads={}",
                n,
                self.rg.nvars,
                M.len(),
                rayon::current_num_threads()
            );
            #[cfg(not(feature = "parallel"))]
            println!(
                "[LF+ Cm::prove] start: n={} nvars={} Mlen={} rayon_threads=DISABLED(feature=parallel)",
                n,
                self.rg.nvars,
                M.len(),
            );
        }

        let t = Instant::now();
        let dcom = self.rg.range_check(M, transcript);
        if profile {
            println!("[LF+ Cm::prove] range_check: {:?}", t.elapsed());
        }

        let s = (0..3)
            .map(|_| short_challenge(128, transcript))
            .collect::<Vec<R>>();

        let s_prime = (0..k)
            .map(|_| {
                (0..d)
                    .map(|_| short_challenge(128, transcript))
                    .collect::<Vec<R>>()
            })
            .collect::<Vec<_>>();
        let s_prime_flat = s_prime.clone().into_iter().flatten().collect::<Vec<R>>();

        let t = Instant::now();
        let h: Vec<Vec<R>> = self
            .rg
            .instances
            .iter()
            .map(|inst| {
                let n = 1 << self.rg.nvars;
                let h_vectors: Vec<Vec<R>> = inst
                    .M_f
                    .iter()
                    .zip(s_prime.iter())
                    .map(|(M, s_i)| {
                        debug_assert_eq!(M.nrows, n);
                        debug_assert_eq!(M.ncols, s_i.len());
                        #[cfg(feature = "parallel")]
                        {
                            use rayon::prelude::*;
                            (0..n)
                                .into_par_iter()
                                .map(|row| {
                                    let mut acc = R::ZERO;
                                    for col in 0..M.ncols {
                                        acc += M.get(row, col) * s_i[col];
                                    }
                                    acc
                                })
                                .collect::<Vec<_>>()
                        }
                        #[cfg(not(feature = "parallel"))]
                        {
                            let mut out = vec![R::ZERO; n];
                            for row in 0..n {
                                let mut acc = R::ZERO;
                                for col in 0..M.ncols {
                                    acc += M.get(row, col) * s_i[col];
                                }
                                out[row] = acc;
                            }
                            out
                        }
                    })
                    .collect();

                let mut h = vec![R::zero(); n];
                for v in h_vectors {
                    for (i, val) in v.iter().enumerate() {
                        h[i] += *val;
                    }
                }
                h
            })
            .collect();
        if profile {
            println!("[LF+ Cm::prove] build h: {:?}", t.elapsed());
        }

        let t = Instant::now();
        let comh: Vec<Vec<R>> = self
            .rg
            .instances
            .iter()
            .map(|inst| {
                let comh_vectors = inst
                    .comM_f
                    .iter()
                    .zip(s_prime.iter())
                    .map(|(comM_f_i, s_i)| comM_f_i.try_mul_vec(s_i).unwrap())
                    .collect::<Vec<_>>();

                let mut comh = vec![R::zero(); inst.comM_f[0].nrows];
                for v in comh_vectors {
                    for (i, val) in v.iter().enumerate() {
                        comh[i] += *val;
                    }
                }
                comh
            })
            .collect();
        if profile {
            println!("[LF+ Cm::prove] build comh: {:?}", t.elapsed());
        }

        absorb_comh(&comh, transcript);

        let kappa = comh[0].len();
        let log_kappa = log2(kappa) as usize;

        let c = (0..2)
            .map(|_| {
                transcript
                    .get_challenges(log_kappa)
                    .into_iter()
                    .map(|x| x.into())
                    .collect::<Vec<R>>()
            })
            .collect::<Vec<_>>();

        let dpp = (0..l)
            .map(|i| R::from(R::BaseRing::from(dp as u128).pow([i as u64])))
            .collect::<Vec<_>>();
        let xp = (0..d).map(|i| unit_monomial::<R>(i)).collect::<Vec<_>>();

        // Build *structured* tensor tables without materializing O(n) vectors.
        let t = Instant::now();
        let tensor_c0 = crate::utils::tensor(&c[0]);
        let tensor_c1 = crate::utils::tensor(&c[1]);
        let tensor_len = tensor_c0.len() * s_prime_flat.len() * dpp.len() * xp.len();
        assert_eq!(tensor_c0.len(), tensor_c1.len());
        if tensor_len > n {
            panic!("t(z) tensor_len {} > n {}", tensor_len, n);
        }
        let t0_mle = StreamingMleEnum::Tensor4Padded {
            t1: Arc::new(tensor_c0),
            t2: Arc::new(s_prime_flat.clone()),
            t3: Arc::new(dpp.clone()),
            t4: Arc::new(xp.clone()),
            tensor_len,
            num_vars: self.rg.nvars,
        };
        let t1_mle = StreamingMleEnum::Tensor4Padded {
            t1: Arc::new(tensor_c1),
            t2: Arc::new(s_prime_flat.clone()),
            t3: Arc::new(dpp.clone()),
            t4: Arc::new(xp.clone()),
            tensor_len,
            num_vars: self.rg.nvars,
        };
        if profile {
            println!(
                "[LF+ Cm::prove] build t(z) streaming: {:?} (tensor_len={}, padded_to_n={})",
                t.elapsed(),
                tensor_len,
                n
            );
        }

        let (proof_a, evals_a, ro_a) = self.sumchecker_streaming(&dcom, &h, &t0_mle, &t1_mle, M, transcript, profile);
        let (proof_b, evals_b, ro_b) = self.sumchecker_streaming(&dcom, &h, &t0_mle, &t1_mle, M, transcript, profile);

        // Step 7
        // TODO needs more folding challenges `s` for the L instances
        let g = self
            .rg
            .instances
            .iter()
            .enumerate()
            .map(|(i, inst)| {
                inst.tau
                    .iter()
                    .zip(&inst.m_tau)
                    .zip(&inst.f)
                    .zip(&h[i])
                    .map(|(((r_tau, r_mtau), r_f), r_h)| {
                        (s[0] * R::from(*r_tau)) + (s[1] * r_mtau) + (s[2] * r_f) + r_h
                    })
                    .collect::<Vec<R>>()
            })
            .collect::<Vec<_>>();

        let proof = CmProof {
            dcom,
            comh,
            sumcheck_proofs: (proof_a, proof_b),
            evals: (evals_a, evals_b),
        };

        let ro = ro_a.into_iter().zip(ro_b).collect::<Vec<_>>();

        let x = proof.x(&s, ro);

        let com = Com { g, x };

        if profile {
            println!("[LF+ Cm::prove] total: {:?}", t_total.elapsed());
        }

        (com, proof)
    }

    fn sumchecker_streaming(
        &self,
        dcom: &Dcom<R>,
        h: &[Vec<R>],
        t0_mle: &StreamingMleEnum<R>,
        t1_mle: &StreamingMleEnum<R>,
        M: &[SparseMatrix<R>],
        transcript: &mut impl Transcript<R>,
        profile: bool,
    ) -> (Proof<R>, Vec<InstanceEvals<R>>, Vec<R>) {
        let t_sumcheck = Instant::now();
        let nvars = self.rg.nvars;

        let rc = transcript.get_challenge();

        let L = self.rg.instances.len();

        let mut mles = Vec::with_capacity(
            1 // eq
            + L * (
                4  // [tau, m_tau, f, h]
                + 4 * M.len() // M * [tau, ...]
            )
            + 2, // t(z)
        );

        // eq table as structured base-ring MLE.
        let r0 = dcom.out.r.clone();
        let one_minus_r0 = r0.iter().copied().map(|x| R::BaseRing::ONE - x).collect();
        mles.push(StreamingMleEnum::EqBase {
            scale: R::BaseRing::ONE,
            r: r0,
            one_minus_r: one_minus_r0,
        });

        // Share `M` matrices (clone each once into an Arc, instead of materializing M*w vectors).
        let m_arcs: Vec<Arc<SparseMatrix<R>>> = M.iter().cloned().map(Arc::new).collect();

        for (i, inst) in self.rg.instances.iter().enumerate() {
            let tau0 = StreamingMleEnum::BaseScalarOwned {
                evals: inst.tau.clone(),
                num_vars: nvars,
            };
            // NOTE: these clones existed before (dense MLE construction); streaming just avoids the *derived* tables.
            let m_tau_arc = Arc::new(inst.m_tau.clone());
            let f_arc = Arc::new(inst.f.clone());
            let h_arc = Arc::new(h[i].clone());
            let m_tau = StreamingMleEnum::DenseArc {
                evals: m_tau_arc.clone(),
                num_vars: nvars,
            };
            let f_mle = StreamingMleEnum::DenseArc {
                evals: f_arc.clone(),
                num_vars: nvars,
            };
            let h_mle = StreamingMleEnum::DenseArc {
                evals: h_arc.clone(),
                num_vars: nvars,
            };

            mles.push(tau0);
            mles.push(m_tau);
            mles.push(f_mle);
            mles.push(h_mle);

            // Materialize tau as ring only once for sparse mat-vec evaluation.
            let tau_ring: Vec<R> = inst.tau.iter().copied().map(R::from).collect();
            let tau_ring = Arc::new(tau_ring);

            for m in &m_arcs {
                mles.push(StreamingMleEnum::SparseMatVec {
                    matrix: m.clone(),
                    witness: tau_ring.clone(),
                    num_vars: nvars,
                });
                mles.push(StreamingMleEnum::SparseMatVec {
                    matrix: m.clone(),
                    witness: m_tau_arc.clone(),
                    num_vars: nvars,
                });
                mles.push(StreamingMleEnum::SparseMatVec {
                    matrix: m.clone(),
                    witness: f_arc.clone(),
                    num_vars: nvars,
                });
                mles.push(StreamingMleEnum::SparseMatVec {
                    matrix: m.clone(),
                    witness: h_arc.clone(),
                    num_vars: nvars,
                });
            }
        }

        mles.push(t0_mle.clone());
        mles.push(t1_mle.clone());

        let Mlen = M.len();

        // Pre-compute random-combinator powers
        let mut rcps = vec![];
        let mut rcp = R::BaseRing::ONE;
        for _ in 0..L {
            // [tau, m_tau, f, h]
            for _ in 0..4 {
                rcps.push(rcp);
                rcp *= rc;
            }
            for _ in 0..Mlen {
                // M_i * [tau, m_tau, f, h]
                for _ in 0..4 {
                    rcps.push(rcp);
                    rcp *= rc;
                }
            }
        }
        rcps.push(rcp); // t(0)
        rcp *= rc;
        rcps.push(rcp); // t(1)

        let comb_fn = |vals: &[R]| -> R {
            (0..L)
                .map(|l| {
                    let l_idx = 1 + l * (4 + 4 * Mlen);
                    vals[0] * ( // eq
                    vals[l_idx] * rcps[l_idx - 1]  // tau
                    + vals[l_idx + 1] * rcps[l_idx] // m_tau
                    + vals[l_idx + 2] * rcps[l_idx + 1] // f
                    + vals[l_idx + 3] * rcps[l_idx + 2] // h
                    + (0..Mlen).map(|i| {
                        let idx = l_idx + 4 + i * 4;
                        vals[idx] * rcps[idx - 1] // M_i * tau
                        + vals[idx + 1] * rcps[idx] // M_i * m_tau
                        + vals[idx + 2] * rcps[idx + 1] // M_i * f
                        + vals[idx + 3] * rcps[idx + 2] // M_i * h
                     }).sum::<R>()
                )
            + (vals[l_idx] * vals[vals.len()-2]) * rcps[vals.len() - 3] // t(0)
            + (vals[l_idx] * vals[vals.len()-1]) * rcps[vals.len() - 2] // t(1)
                })
                .sum::<R>()
        };

        let (sumcheck_proof, randomness, final_vals) =
            StreamingSumcheck::prove_as_subprotocol(transcript, mles, nvars, 2, comb_fn);

        let ro = randomness.into_iter().map(|x| x.into()).collect::<Vec<R>>();

        let evals = (0..L)
            .map(|l| {
                let mut e = Vec::with_capacity(1 + Mlen);
                let l_idx = 1 + l * (4 + 4 * Mlen);
                e.push([
                    final_vals[l_idx],
                    final_vals[l_idx + 1],
                    final_vals[l_idx + 2],
                    final_vals[l_idx + 3],
                ]);
                for i in 0..Mlen {
                    let idx = l_idx + 4 + i * 4;
                    e.push([
                        final_vals[idx],
                        final_vals[idx + 1],
                        final_vals[idx + 2],
                        final_vals[idx + 3],
                    ]);
                }
                InstanceEvals(e)
            })
            .collect::<Vec<_>>();

        absorb_evaluations(&evals, transcript);

        if profile {
            println!(
                "[LF+ Cm::sumchecker_streaming] sumcheck+evals: {:?} (mles={}, L={}, Mlen={})",
                t_sumcheck.elapsed(),
                final_vals.len(),
                L,
                Mlen
            );
        }

        (sumcheck_proof, evals, ro)
    }
}

impl<R: CoeffRing> CmProof<R>
where
    R::BaseRing: Zq,
{
    pub fn verify(
        &self,
        M: &[SparseMatrix<R>],
        transcript: &mut impl Transcript<R>,
    ) -> Result<ComX<R>, SumCheckError<R>> {
        let k = self.dcom.dparams.k;
        let d = R::dimension();
        let nvars = self.dcom.out.nvars;
        let L = self.evals.0.len();

        self.dcom.verify(transcript).unwrap();

        let s = (0..3)
            .map(|_| short_challenge(128, transcript))
            .collect::<Vec<R>>();

        let s_prime = (0..k)
            .map(|_| {
                (0..d)
                    .map(|_| short_challenge(128, transcript))
                    .collect::<Vec<R>>()
            })
            .collect::<Vec<_>>();
        let s_prime_flat = s_prime.clone().into_iter().flatten().collect::<Vec<R>>();

        absorb_comh(&self.comh, transcript);

        let kappa = self.comh[0].len();
        let log_kappa = log2(kappa) as usize;

        let c = (0..2)
            .map(|_| {
                transcript
                    .get_challenges(log_kappa)
                    .into_iter()
                    .map(|x| x.into())
                    .collect::<Vec<R>>()
            })
            .collect::<Vec<_>>();

        let u: Vec<Vec<R>> = (0..L)
            .map(|l| {
                self.dcom
                    .out
                    .e
                    .iter()
                    .map(|e_i| {
                        e_i.iter()
                            .skip(l * k)
                            .take(k)
                            .flatten()
                            .zip(s_prime_flat.iter())
                            .map(|(u_ij, s_ij)| *u_ij * *s_ij)
                            .sum()
                    })
                    .collect::<Vec<R>>()
            })
            .collect();

        let tensor_c0 = tensor(&c[0]);
        let tensor_c1 = tensor(&c[1]);
        let tcch0 = self
            .comh
            .iter()
            .map(|com| {
                tensor_c0
                    .iter()
                    .zip(com)
                    .map(|(&t_i, ch_i)| t_i * ch_i)
                    .sum::<R>()
            })
            .collect::<Vec<R>>();
        let tcch1 = self
            .comh
            .iter()
            .map(|com| {
                tensor_c1
                    .iter()
                    .zip(com)
                    .map(|(&t_i, ch_i)| t_i * ch_i)
                    .sum::<R>()
            })
            .collect::<Vec<R>>();

        let dp = R::dimension() / 2;
        let l = self.dcom.dparams.l;
        let dpp = (0..l)
            .map(|i| R::from(R::BaseRing::from(dp as u128).pow([i as u64])))
            .collect::<Vec<_>>();
        let xp = (0..d).map(|i| unit_monomial::<R>(i)).collect::<Vec<_>>();

        let mut verify_sumcheck =
            |sumcheck_proof: &Proof<R>, evals: &[InstanceEvals<R>]| -> Result<Vec<R>, ()> {
                let rc: R = transcript.get_challenge().into();

                let z_idx = L * (4 + 4 * M.len());

                let claimed_sum = self
                    .dcom
                    .evals
                    .iter()
                    .enumerate()
                    .map(|(l, eval)| {
                        let l_idx = l * (4 + 4 * M.len());

                        R::from(eval.a[0]) * rc.pow([l_idx as u64])
                            + eval.b[0] * rc.pow([l_idx as u64 + 1])
                            + eval.c[0] * rc.pow([l_idx as u64 + 2])
                            + u[l][0] * rc.pow([l_idx as u64 + 3])
                            + (0..M.len())
                                .map(|i| {
                                    let idx = l_idx + 4 + i * 4;
                                    R::from(eval.a[1 + i]) * rc.pow([idx as u64])
                                        + eval.b[1 + i] * rc.pow([idx as u64 + 1])
                                        + eval.c[1 + i] * rc.pow([idx as u64 + 2])
                                        + u[l][1 + i] * rc.pow([idx as u64 + 3])
                                })
                                .sum::<R>()
                            + tcch0[l] * rc.pow([z_idx as u64])
                            + tcch1[l] * rc.pow([z_idx as u64 + 1])
                    })
                    .sum::<R>();

                let subclaim = MLSumcheck::verify_as_subprotocol(
                    transcript,
                    nvars,
                    2,
                    claimed_sum,
                    sumcheck_proof,
                )
                .unwrap();

                let r: Vec<R> = self.dcom.out.r.iter().map(|x| R::from(*x)).collect();
                let ro: Vec<R> = subclaim.point.into_iter().map(|x| x.into()).collect();
                
                // OPTIMIZED: Use tensor structure for O(small) evaluation instead of O(n)
                // The tensor product t(z) = tensor(c_z) ⊗ s' ⊗ d_powers ⊗ x_powers
                // can be evaluated factor-by-factor in O(κ + k*d + ℓ + d) time.
                use crate::tensor_eval::eval_t_z_optimized;
                let t0_ro = eval_t_z_optimized(&c[0], &s_prime_flat, &dpp, &xp, &ro);
                let t1_ro = eval_t_z_optimized(&c[1], &s_prime_flat, &dpp, &xp, &ro);

                let expected_eval = subclaim.expected_evaluation;

                absorb_evaluations(evals, transcript);

                let eq = eq_eval(&r, &ro).unwrap();

                let eval = evals
                    .iter()
                    .enumerate()
                    .map(|(l, el)| {
                        let el = &el.0;
                        let l_idx = l * (4 + 4 * M.len());
                        eq * (el[0][0] * rc.pow([l_idx as u64])
                            + el[0][1] * rc.pow([l_idx as u64 + 1])
                            + el[0][2] * rc.pow([l_idx as u64 + 2])
                            + el[0][3] * rc.pow([l_idx as u64 + 3])
                            + (0..M.len())
                                .map(|i| {
                                    // M_i
                                    let M_evals = el[i + 1];
                                    let idx = l_idx + 4 + i * 4;
                                    M_evals[0] * rc.pow([idx as u64])
                                        + M_evals[1] * rc.pow([idx as u64 + 1])
                                        + M_evals[2] * rc.pow([idx as u64 + 2])
                                        + M_evals[3] * rc.pow([idx as u64 + 3])
                                })
                                .sum::<R>())
                            + (t0_ro * el[0][0]) * rc.pow([z_idx as u64])
                            + (t1_ro * el[0][0]) * rc.pow([z_idx as u64 + 1])
                    })
                    .sum::<R>();

                assert_eq!(expected_eval, eval);

                Ok(ro)
            };

        let ro0 = verify_sumcheck(&self.sumcheck_proofs.0, &self.evals.0).unwrap();
        let ro1 = verify_sumcheck(&self.sumcheck_proofs.1, &self.evals.1).unwrap();

        let ro = ro0.into_iter().zip(ro1).collect::<Vec<_>>();

        // Step 6
        Ok(self.x(&s, ro))
    }

    pub fn x(&self, s: &[R], ro: Vec<(R, R)>) -> ComX<R> {
        let L = self.dcom.fcoms.len();

        // TODO needs more folding challenges `s` for the L instances
        let cm_g = self
            .dcom
            .fcoms
            .iter()
            .enumerate()
            .map(|(l, cmc)| {
                cmc.C_Mf
                    .iter()
                    .zip(&cmc.cm_mtau)
                    .zip(&cmc.cm_f)
                    .zip(&self.comh[l])
                    .map(|(((r_Mf, r_mtau), r_f), r_comh)| {
                        s[0] * r_Mf + s[1] * r_mtau + s[2] * r_f + r_comh
                    })
                    .collect::<Vec<R>>()
            })
            .collect::<Vec<_>>();

        let vo = (0..L)
            .map(|l| {
                let e0l = &self.evals.0[l].0;
                let e1l = &self.evals.1[l].0;
                e0l.iter()
                    .zip(e1l.iter())
                    .map(|(e0li, e1li)| {
                        (
                            (s[0] * e0li[0]) + (s[1] * e0li[1]) + (s[2] * e0li[2]) + e0li[3],
                            (s[0] * e1li[0]) + (s[1] * e1li[1]) + (s[2] * e1li[2]) + e1li[3],
                        )
                    })
                    .collect::<Vec<(R, R)>>()
            })
            .collect::<Vec<Vec<_>>>();

        ComX { cm_g, ro, vo }
    }
}

fn absorb_comh<R: OverField>(comh: &[Vec<R>], transcript: &mut impl Transcript<R>) {
    comh.iter().for_each(|ci| transcript.absorb_slice(ci));
}

fn absorb_evaluations<R: OverField>(
    evals: &[InstanceEvals<R>],
    transcript: &mut impl Transcript<R>,
) {
    evals.iter().for_each(|ieval| {
        ieval.0.iter().for_each(|vals| {
            transcript.absorb_slice(vals);
        });
    });
}

/// t(z) = tensor(c(z)) ⊗ s' ⊗ (1, d', ..., d'^(ℓ-1)) ⊗ (1, X, ..., X^(d-1))
#[allow(dead_code)]
// Dense reference implementation (debugging / cross-checking).
// Hot paths use streaming `Tensor4Padded` (prover) and `eval_t_z_optimized` (verifier).
fn calculate_t_z<T>(c_z: &[T], s_prime: &[T], d_prime_powers: &[T], x_powers: &[T]) -> Vec<T>
where
    T: Clone + One + Sub<Output = T> + Mul<Output = T>,
{
    let tensor_c_z = tensor(c_z);
    let part1 = tensor_product(&tensor_c_z, s_prime);
    let part2 = tensor_product(&part1, d_prime_powers);
    tensor_product(&part2, x_powers)
}

#[cfg(test)]
mod tests {
    use ark_ff::PrimeField;
    use ark_std::Zero;
    use cyclotomic_rings::rings::FrogPoseidonConfig as PC;
    use stark_rings::cyclotomic_ring::models::frog_ring::RqPoly as R;
    use stark_rings_linalg::{Matrix, SparseMatrix};

    use super::*;
    use crate::{
        rgchk::{DecompParameters, RgInstance},
        transcript::PoseidonTranscript,
    };

    #[test]
    fn test_com() {
        // f: [
        // 2 + 5X
        // 4 + X^2
        // ]
        let n = 1 << 15;
        let mut f = vec![R::zero(); n];
        f[0].coeffs_mut()[0] = 2u128.into();
        f[0].coeffs_mut()[1] = 5u128.into();
        f[1].coeffs_mut()[0] = 4u128.into();
        f[1].coeffs_mut()[2] = 1u128.into();

        let mut m = SparseMatrix::identity(n);
        m.coeffs[0][0].0 = 2u128.into();
        let M = vec![m];

        let kappa = 2;
        let b = (R::dimension() / 2) as u128;
        let k = 2;
        // log_d' (q)
        let l = ((<<R as PolyRing>::BaseRing>::MODULUS.0[0] as f64).ln()
            / ((R::dimension() / 2) as f64).ln())
        .ceil() as usize;

        let A = Matrix::<R>::rand(&mut ark_std::test_rng(), kappa, n);

        let dparams = DecompParameters { b, k, l };
        let instance = RgInstance::from_f(f.clone(), &A, &dparams);

        let rg = Rg {
            nvars: log2(n) as usize,
            instances: vec![instance],
            dparams: DecompParameters { b, k, l },
        };

        let cm = Cm { rg };

        let mut ts = PoseidonTranscript::empty::<PC>();
        let (_com, proof) = cm.prove(&M, &mut ts);

        let mut ts = PoseidonTranscript::empty::<PC>();
        proof.verify(&M, &mut ts).unwrap();
    }
}
